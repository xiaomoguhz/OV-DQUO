Metadata-Version: 2.1
Name: open_clip_torch
Version: 2.16.0
Summary: OpenCLIP
Home-page: https://github.com/mlfoundations/open_clip
Author: 
Author-email: 
Keywords: CLIP pretrained
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Topic :: Scientific/Engineering
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development
Classifier: Topic :: Software Development :: Libraries
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Requires-Python: >=3.7
Description-Content-Type: text/markdown
Requires-Dist: lvis==0.5.3
Requires-Dist: mmcv-full==1.7.1
Requires-Dist: mmdet==2.28.1
Requires-Dist: mmengine==0.8.5
Requires-Dist: opencv-python==4.8.0.76
Requires-Dist: Pillow==10.0.0
Requires-Dist: pycocotools==2.0.7
Requires-Dist: timm==0.9.7
Requires-Dist: tokenizers==0.13.3
Requires-Dist: transformers==4.33.0
Requires-Dist: triton==2.0.0
Requires-Dist: webdataset>=0.2.5
Requires-Dist: regex
Requires-Dist: ftfy
Requires-Dist: tqdm
Requires-Dist: pandas
Requires-Dist: braceexpand
Requires-Dist: huggingface_hub
Requires-Dist: fsspec

# OV-DQUO: Open-Vocabulary DETR with Denoising Text Query Training and Open-World Unknown Objects Supervision
This repository is the official implementation of [OV-DQUO](https://arxiv.org/abs/2030.12345). 



![Alt text](docs/method.png)

## Environment
### Requirements
- Linux with Python == 3.9.0
- CUDA 11.7
- The provided environment is suggested for reproducing our results, similar configurations may also work.

### Quick Start

#### create conda environment
```
conda create -n OV-DQUO python=3.9.0
conda activate OV-DQUO
pip install torch==2.0.0 torchvision==0.15.1
```
#### install OpenCLIP 
`pip install -e . -v`
#### build for DeformableAttention 
```
cd ./models/ops
sh ./make.sh
```

>ðŸ“‹  Describe how to train the models, with example commands on how to train the models in your paper, including the full training procedure and appropriate hyperparameters.


## Results & Pre-trained Models  
Our model achieves the following performance on :
### OV-COCO

| Model name    | __AP50_Novel__  | AP50_All | Checkpoint |
| ------------  | :------------: | :------------: | ------------ |
| OVDQUO_RN50   | __39.2__ | 41.8 |             |
| OVDQUO_RN50x4 | __45.6__ | 48.1 |  |

### OV-LVIS
| Model name    | mAP_rare     | mAP_All | Checkpoint |
| ------------  | :------------: | :------------: | ------------ |
| OVDQUO_ViTB16 | __29.7__ |   |                  |
| OVDQUO_ViTL14 | __39.3__ |   |                  |
## Evaluation

To evaluate my model on OV-COCO, run:

```eval
python eval.py --model-file mymodel.pth --benchmark imagenet
```
To evaluate my model on OV-LVIS, run:

```eval
python eval.py --model-file mymodel.pth --benchmark imagenet
```
>ðŸ“‹  Describe how to evaluate the trained models on benchmarks reported in the paper, give commands that produce the results (section below).

## Citation and Acknowledgement

### Citation

If you find this repo useful, please consider citing our paper:

<!-- ```
@inproceedings{wu2023cora,
  title={CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching},
  author={Xiaoshi Wu and Feng Zhu and Rui Zhao and Hongsheng Li},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={TBD--TBD},
  year={2023}
}
``` -->

```
@article{wu2023cora,
  title={CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting and Anchor Pre-Matching},
  author={Xiaoshi Wu and Feng Zhu and Rui Zhao and Hongsheng Li},
  journal={ArXiv},
  year={2023},
  volume={abs/2303.13076}
}
```

### Acknowledgement

This repository was built on top of [SAM-DETR](https://github.com/ZhangGongjie/SAM-DETR), [CLIP](https://github.com/openai/CLIP), [RegionClip](https://github.com/microsoft/RegionCLIP), and [DAB-DETR](https://github.com/IDEA-Research/DAB-DETR). We thank the effort from the community.
